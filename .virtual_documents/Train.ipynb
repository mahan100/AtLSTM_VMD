





# Parameters
SEQ_LEN = 24  # past 24 hours for each sample
TARGET_COL = 'OT'  # target variable: Oil Temperature
EPOCHS = 30 
BATCH_SIZE = 32

# VMD parameters
DC = 0              # no DC part imposed
init = 1            # initialize omegas uniformly
tol = 1e-7


import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import os
from vmdpy import VMD
import pandas as pd
from sklearn.preprocessing import MinMaxScaler


save_dir = os.path.expanduser("~/Project/AttnLstm/data/raw")
os.makedirs(save_dir, exist_ok=True)
# Full file path
file_path = os.path.join(save_dir, "ETTh1.csv")
# Load it whenever needed
df = pd.read_csv(file_path)
print(df.head())
df.columns = df.columns.str.strip().str.replace('\ufeff', '')
print(df.columns)
# Ensure datetime type
df['date'] = pd.to_datetime(df['date'])
df.set_index('date', inplace=True)
data = df[[TARGET_COL]].values



# Visualize
plt.figure(figsize=(12, 4))
plt.plot(df.index, df['OT'], label='Oil Temperature (OT)')
plt.title('ETTh1 - Oil Temperature Time Series')
plt.xlabel('Date')
plt.ylabel('OT')
plt.legend()
plt.show()





# Select relevant columns
df_model = df[['OT']].copy()

# Split index (80% train, 20% test)
split_idx = int(len(df_model) * 0.8)

# Split data chronologically
train_data = df_model[:split_idx]
test_data  = df_model[split_idx:]

# Fit scaler only on training data
scaler = MinMaxScaler()
train_scaled = scaler.fit_transform(train_data)
test_scaled  = scaler.transform(test_data)

# Sequence creation function
def create_sequences(data, seq_len):
    X, y = [], []
    for i in range(len(data) - seq_len):
        X.append(data[i:i + seq_len])
        y.append(data[i + seq_len, 0])  # target: OT
    return np.array(X), np.array(y)

# Create sequences
X_train_simple, y_train_simple = create_sequences(train_scaled, SEQ_LEN)
X_test_simple, y_test_simple   = create_sequences(test_scaled, SEQ_LEN)
y_test_real = scaler.inverse_transform(y_test_simple.reshape(-1, 1)).flatten()

# If using pure LSTM on OT only
X_train_simple = X_train_simple[:, :, 0:1]
X_test_simple  = X_test_simple[:, :, 0:1]

print(f"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}")
print(f"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}")






import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Attention, Concatenate
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
from vmdpy import VMD  # make sure: pip install vmdpy
import random

# =====================================================
# 1Ô∏è‚É£ ATT-LSTM Model
# =====================================================
def build_attlstm_model(input_shape):
    inp = Input(shape=input_shape)
    lstm_out = LSTM(128, return_sequences=True)(inp)
    lstm_out = Dropout(0.2)(lstm_out)

    # Attention
    query = Dense(128)(lstm_out)
    value = Dense(128)(lstm_out)
    attention_out = Attention()([query, value])
    attention_out = Concatenate()([lstm_out, attention_out])

    # Decoder
    lstm_dec = LSTM(64, return_sequences=False)(attention_out)
    dense_out = Dense(32, activation='relu')(lstm_dec)
    final_out = Dense(1)(dense_out)  # predict real OT

    model = Model(inputs=inp, outputs=final_out)
    model.compile(optimizer=Adam(1e-3), loss='mse', metrics=['mae'])
    return model

# =====================================================
# 2Ô∏è‚É£ Sequence Creator
# =====================================================
def create_sequences(X, y, seq_len):
    Xs, ys = [], []
    for i in range(len(X) - seq_len):
        Xs.append(X[i:i+seq_len])
        ys.append(y[i+seq_len])
    return np.array(Xs), np.array(ys)

# =====================================================
# 3Ô∏è‚É£ Fitness Function for GA
# =====================================================
def fitness(params, data, seq_len=SEQ_LEN, epochs=EPOCHSO):
    alpha, tau, K = params['alpha'], params['tau'], int(params['K'])
    signal = data['OT'].values.flatten()

    # VMD decomposition
    u, _, _ = VMD(signal, alpha, tau, K, DC=0, init=1, tol=1e-7)
    vmd_features = np.stack(u, axis=1)

    # Scale features and target
    scaler_x = MinMaxScaler()
    scaler_y = MinMaxScaler()
    X_scaled = scaler_x.fit_transform(vmd_features)
    y_scaled = scaler_y.fit_transform(signal.reshape(-1, 1))

    # Split (80/20)
    split = int(len(X_scaled) * 0.8)
    X_train, X_test = X_scaled[:split], X_scaled[split:]
    y_train, y_test = y_scaled[:split], y_scaled[split:]

    # Create sequences
    X_train_seq, y_train_seq = create_sequences(X_train, y_train, seq_len)
    X_test_seq, y_test_seq = create_sequences(X_test, y_test, seq_len)

    # Build & Train
    model = build_attlstm_model((seq_len, K))
    history = model.fit(
        X_train_seq, y_train_seq,
        validation_data=(X_test_seq, y_test_seq),
        epochs=epochs,
        batch_size=32,
        verbose=0
    )

    # Predict and inverse-transform
    y_pred_scaled = model.predict(X_test_seq)
    y_pred = scaler_y.inverse_transform(y_pred_scaled)
    y_true = scaler_y.inverse_transform(y_test_seq)

    mse = np.mean((y_true - y_pred) ** 2)
    return mse, history, model, (y_true, y_pred)

# =====================================================
# 4Ô∏è‚É£ Genetic Algorithm (GA)
# =====================================================
def random_params():
    return {
        'alpha': random.uniform(1000, 3000),
        'tau': random.uniform(0.0, 0.5),
        'K': random.randint(3, 8)
    }

def mutate(params):
    new_params = params.copy()
    if random.random() < 0.5:
        new_params['alpha'] *= random.uniform(0.9, 1.1)
    if random.random() < 0.5:
        new_params['tau'] += random.uniform(-0.05, 0.05)
    if random.random() < 0.3:
        new_params['K'] = max(2, min(10, new_params['K'] + random.choice([-1, 1])))
    return new_params

def evolve_population(data, pop_size=4, generations=3):
    population = [random_params() for _ in range(pop_size)]

    for g in range(generations):
        print(f"\nüß¨ Generation {g+1}/{generations}")
        scores = []
        for i, params in enumerate(population):
            mse, _, _, _ = fitness(params, data)
            scores.append((mse, params))
            print(f"  Candidate {i+1}: MSE = {mse:.4f}, params = {params}")

        scores.sort(key=lambda x: x[0])
        best_params = scores[0][1]
        print(f"  ‚úÖ Best so far: {best_params}")

        # Select top 2 and mutate
        new_pop = [best_params]
        new_pop.append(scores[1][1])
        while len(new_pop) < pop_size:
            parent = random.choice(new_pop)
            new_pop.append(mutate(parent))
        population = new_pop

    return best_params

# =====================================================
# 5Ô∏è‚É£ Run Optimization
# =====================================================
# df should contain df['OT']
best_params = evolve_population(df, pop_size=3, generations=2)

# =====================================================
# 6Ô∏è‚É£ Retrain with Best Params
# =====================================================
mse, history, model, (y_true_ot, y_pred_ot) = fitness(best_params, df, epochs=20)

print(f"\nFinal MSE with optimized params: {mse:.4f}")
print("Best params:", best_params)

# =====================================================
# 7Ô∏è‚É£ Visualization
# =====================================================
plt.figure(figsize=(8,4))
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title('Training & Validation Loss (ATT-LSTM)')
plt.legend(); plt.grid(True); plt.show()

plt.figure(figsize=(12,5))
plt.plot(y_true_ot, label='True OT', color='black', linewidth=1)
plt.plot(y_pred_ot, label='Predicted OT', color='red', alpha=0.8)
plt.title('True vs Predicted OT (ATT-LSTM + VMD)')
plt.xlabel('Time steps'); plt.ylabel('Oil Temperature')
plt.legend(); plt.grid(True); plt.show()

plt.figure(figsize=(10,4))
residuals = y_true_ot - y_pred_ot
plt.plot(residuals, color='purple')
plt.title('Residuals (True - Predicted)')
plt.grid(True); plt.show()






from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional
from tensorflow.keras.callbacks import EarlyStopping

# Advanced LSTM model
advanced_lstm = Sequential([
    Bidirectional(LSTM(122, return_sequences=True), input_shape=(SEQ_LEN, 1)),
    Dropout(0.2),
    LSTM(122, return_sequences=True),
    LSTM(64, return_sequences=True),
    Dropout(0.2),
    LSTM(122, return_sequences=False),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dense(1)
])

advanced_lstm.compile(optimizer='adam', loss='mse')
advanced_lstm.summary()

# Early stopping
early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Train model
history_adv = advanced_lstm.fit(
    X_train_simple, y_train_simple,
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    validation_data=(X_test_simple, y_test_simple),
    callbacks=[early_stop],
    verbose=1
)

# Plot training history
plt.figure(figsize=(8, 4))
plt.plot(history_adv.history['loss'], label='Training Loss')
plt.plot(history_adv.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('MSE Loss')
plt.title('Advanced LSTM Training Performance')
plt.legend()
plt.grid(True)
plt.show()

# Make predictions
y_pred_scaled_adv = advanced_lstm.predict(X_test_simple)
y_pred_adv = scaler.inverse_transform(y_pred_scaled_adv.reshape(-1, 1)).flatten()

# Plot predictions vs true values
plt.figure(figsize=(12, 5))
plt.plot(y_test_real, label='True OT', color='blue')
plt.plot(y_pred_adv, label='Predicted OT', color='green')
plt.xlabel('Time Steps')
plt.ylabel('Oil Temperature')
plt.title('Advanced LSTM Predictions vs True Values')
plt.legend()
plt.grid(True)
plt.show()






from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np

def compute_metrics(y_true, y_pred):
    min_len = min(len(y_true), len(y_pred))
    y_true, y_pred = y_true[:min_len], y_pred[:min_len]
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    return mse, rmse, mae, r2

# Compute metrics
mse_vmd, rmse_vmd, mae_vmd, r2_vmd = compute_metrics(y_true_vmd_ot, y_test_real)
mse_adv, rmse_adv, mae_adv, r2_adv = compute_metrics(y_pred_adv, y_test_real)

# Print comparison
print("üìà Model Performance Comparison")
print(f"ATT-LSTM (VMD): MSE={mse_vmd:.4f}, RMSE={rmse_vmd:.4f}, MAE={mae_vmd:.4f}, R¬≤={r2_vmd:.4f}")
print(f"Advanced BiLSTM: MSE={mse_adv:.4f}, RMSE={rmse_adv:.4f}, MAE={mae_adv:.4f}, R¬≤={r2_adv:.4f}")


import matplotlib.pyplot as plt

plt.figure(figsize=(12, 5))
plt.plot(y_test_real, label='True OT', color='blue')
plt.plot(y_pred_vmd_ot, label='Predicted OT (ATT-LSTM+VMD)', color='red')
plt.plot(y_pred_adv[:len(y_test_real)], label='Predicted OT (Advanced BiLSTM)', color='green')
plt.xlabel('Time Steps')
plt.ylabel('Oil Temperature')
plt.title('Model Comparison: ATT-LSTM (VMD) vs BiLSTM')
plt.legend()
plt.grid(True)
plt.show()



# ----------------------------
# üîπ Compare ATT-LSTM (VMD-based) vs Raw OT (simple baseline)
# ----------------------------

# Align lengths for fair comparison
min_len = min(len(y_true), len(y_test_real))
y_true_ot_aligned = y_true[:min_len]
y_test_real = y_test_real[:min_len]

# Plot comparison
plt.figure(figsize=(12,5))
plt.plot(y_true_ot_aligned, label='True OT (from VMD pipeline)', color='black', linewidth=1.2)
plt.plot(y_test_real, label='True OT (simple baseline)', color='blue', alpha=0.7)
plt.title('Comparison: True OT (VMD-based vs Simple Baseline)')
plt.xlabel('Time steps (test set)')
plt.ylabel('Oil Temperature (¬∞C)')
plt.legend()
plt.grid(True)
plt.show()

# Optional: compute difference metrics between them
diff = y_true_ot_aligned - y_test_real
plt.figure(figsize=(10,4))
plt.plot(diff, color='orange')
plt.title('Difference: VMD-true vs Simple-test OT')
plt.xlabel('Time steps')
plt.ylabel('Temperature difference')
plt.grid(True)
plt.show()

